# LSTM Next Word Generation from Corpus

This repository contains an implementation of a Long Short-Term Memory (LSTM) model to generate the next word in a sequence from a given text corpus. The project is developed entirely within a single Jupyter Notebook for ease of understanding and reproducibility.

## Project Overview

The primary objective of this project is to leverage the capabilities of LSTM, a type of Recurrent Neural Network (RNN), to predict the next word in a sequence based on the input text. This is a common task in natural language processing (NLP) with applications in text generation, autocomplete systems, and more.

## Features

- **Data Preprocessing:** Tokenization and preparation of text data for training.
- **Model Building:** Construction and compilation of the LSTM model.
- **Training:** Training the model on the corpus with appropriate hyperparameters.
- **Prediction:** Generating the next word given an input sequence.
